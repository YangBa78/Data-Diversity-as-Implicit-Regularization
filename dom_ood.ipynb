{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from scripts.clip_rn50 import ClipFinetuner, train, CustomDataset, evaluate\n",
    "from scripts.datasets import office_home, convert_bytes_to_images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from vendi_score import image_utils\n",
    "import weightwatcher as ww\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH = \"dataset\"\n",
    "ds = load_dataset(\"wltjr1007/DomainNet\", cache_dir=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clipart_images = ds['train'].filter(lambda example: example['domain'] == 0)\n",
    "train_infograph_images = ds['train'].filter(lambda example: example['domain'] == 1)\n",
    "train_painting_images = ds['train'].filter(lambda example: example['domain'] == 2)\n",
    "train_sketch_images = ds['train'].filter(lambda example: example['domain'] == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipart_images_filtered = [\n",
    "    item for item in train_clipart_images if 0 <= item['label'] <= 33\n",
    "]\n",
    "\n",
    "infograph_images_filtered = [\n",
    "    item for item in train_infograph_images if 0 <= item['label'] <= 33\n",
    "]\n",
    "\n",
    "painting_images_filtered = [\n",
    "    item for item in train_painting_images if 0 <= item['label'] <= 33\n",
    "]\n",
    "\n",
    "sketch_images_filtered = [\n",
    "    item for item in train_sketch_images if 0 <= item['label'] <= 33\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipart_images =  [item['image'] for item in clipart_images_filtered]\n",
    "infograph_images =  [item['image'] for item in infograph_images_filtered]\n",
    "painting_images =  [item['image'] for item in painting_images_filtered]\n",
    "sketch_images =  [item['image'] for item in sketch_images_filtered]\n",
    "\n",
    "\n",
    "clipart_labels =  [item['label'] for item in clipart_images_filtered]\n",
    "infograph_labels =  [item['label'] for item in infograph_images_filtered]\n",
    "painting_labels =  [item['label'] for item in painting_images_filtered]\n",
    "sketch_labels =  [item['label'] for item in sketch_images_filtered]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_clipart_images.features['label'].names[:34]\n",
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 for CLIP\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))  # CLIP's mean and std\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipart_dataset = CustomDataset(images=clipart_images, labels=clipart_labels, classes=classes, transform=transform)\n",
    "infograph_dataset = CustomDataset(images=infograph_images, labels=infograph_labels, classes=classes, transform=transform)\n",
    "painting_dataset = CustomDataset(images=painting_images, labels=painting_labels, classes=classes, transform=transform)\n",
    "sketch_dataset = CustomDataset(images=sketch_images, labels=sketch_labels, classes=classes, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipart_dataloader = DataLoader(clipart_dataset, batch_size=32, shuffle=False)\n",
    "infograph_dataloader = DataLoader(infograph_dataset, batch_size=32, shuffle=False)\n",
    "painting_dataloader = DataLoader(painting_dataset, batch_size=32, shuffle=False)\n",
    "sketch_dataloader = DataLoader(sketch_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ClipFinetuner(num_classes=34)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_acc_clipart = {}\n",
    "ood_loss_clipart = {}\n",
    "ood_ece_clipart = {}\n",
    "\n",
    "ood_acc_infograph = {}\n",
    "ood_loss_infograph = {}\n",
    "ood_ece_infograph = {}\n",
    "\n",
    "ood_acc_painting = {}\n",
    "ood_loss_painting = {}\n",
    "ood_ece_painting = {}\n",
    "\n",
    "ood_acc_sketch = {}\n",
    "ood_loss_sketch = {}\n",
    "ood_ece_sketch = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"ft_model_dp0.1.pth\".split('_')[2].replace('.pth', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataloaders = {\n",
    "    'clipart': clipart_dataloader,\n",
    "    'infograph': infograph_dataloader,\n",
    "    'painting': painting_dataloader,\n",
    "    'sketch': sketch_dataloader\n",
    "}\n",
    "\n",
    "\n",
    "ft_path = \"domainnet/model\"\n",
    "\n",
    "for model_file in os.listdir(ft_path):\n",
    "    if model_file.endswith('.pth'):  # Assuming the model files have .pth extension\n",
    "        model_path = os.path.join(ft_path, model_file)\n",
    "        model_name = model_file.split('_')[2].replace('.pth', '')\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model = model.to('cuda')\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"Evaluating model: {model_name}'\")\n",
    "\n",
    "        for dataset_name, dataloader in dataloaders.items():\n",
    "            test_loss, test_acc, test_ece = evaluate(model, dataloader)\n",
    "            print(f\"{dataset_name.capitalize()}:\")\n",
    "            print(f\"  Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, ECE: {test_ece:.4f}\")\n",
    "            if dataset_name == 'clipart': \n",
    "                ood_acc_clipart[model_name] = test_acc\n",
    "                ood_loss_clipart[model_name] = test_loss\n",
    "                ood_ece_clipart[model_name] = test_ece\n",
    "            \n",
    "            if dataset_name == 'infograph': \n",
    "                ood_acc_infograph[model_name] = test_acc\n",
    "                ood_loss_infograph[model_name] = test_loss\n",
    "                ood_ece_infograph[model_name] = test_ece\n",
    "            if dataset_name == 'painting': \n",
    "                ood_acc_painting[model_name] = test_acc\n",
    "                ood_loss_painting[model_name] = test_loss\n",
    "                ood_ece_painting[model_name] = test_ece\n",
    "            if dataset_name =='sketch':\n",
    "                ood_acc_sketch[model_name] = test_acc\n",
    "                ood_loss_sketch[model_name] = test_loss\n",
    "                ood_ece_sketch[model_name] = test_ece\n",
    "                \n",
    "        print(\"\\n\")  # Add a blank line between models for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_acc_clipart_to = pd.DataFrame([ood_acc_clipart])\n",
    "\n",
    "ood_loss_clipart_to = pd.DataFrame([ood_loss_clipart])\n",
    "\n",
    "ood_ece_clipart_to = pd.DataFrame([ood_ece_clipart])\n",
    "\n",
    "ood_acc_infograph_to = pd.DataFrame([ood_acc_infograph])\n",
    "\n",
    "ood_loss_infograph_to = pd.DataFrame([ood_loss_infograph])\n",
    "\n",
    "ood_ece_infograph_to = pd.DataFrame([ood_ece_infograph])\n",
    "\n",
    "ood_acc_art_to = pd.DataFrame([ood_acc_painting])\n",
    "\n",
    "ood_loss_art_to = pd.DataFrame([ood_loss_painting])\n",
    "\n",
    "ood_ece_art_to = pd.DataFrame([ood_ece_painting])\n",
    "\n",
    "ood_acc_sketch_to = pd.DataFrame([ood_acc_sketch])\n",
    "\n",
    "ood_loss_sketch_to = pd.DataFrame([ood_loss_sketch])\n",
    "\n",
    "ood_ece_sketch_to = pd.DataFrame([ood_ece_sketch])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diversity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
