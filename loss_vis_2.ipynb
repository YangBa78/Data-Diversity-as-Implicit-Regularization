{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from scripts.pre_rn18 import ResNetFinetuner, eigen_train, CustomDataset\n",
    "from scripts.datasets import office_home, convert_bytes_to_images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"flwrlabs/office-home\")\n",
    "\n",
    "class_names, real_data = office_home('real')\n",
    "print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_df = real_data.to_pandas()\n",
    "\n",
    "# Split the in-distribution data into training (80%), validation (10%), and testing (10%)\n",
    "train_val_df, test_df = train_test_split(\n",
    "    real_data_df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=real_data_df['label']\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, \n",
    "    test_size=0.125, \n",
    "    random_state=42, \n",
    "    stratify=train_val_df['label']\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "train_data = Dataset.from_pandas(train_df)\n",
    "val_data = Dataset.from_pandas(val_df)\n",
    "test_data = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(f\"In-Distribution Training Data: {len(train_data)}\")\n",
    "print(f\"In-Distribution Validation Data: {len(val_data)}\")\n",
    "print(f\"In-Distribution Test Data: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_data['image']\n",
    "train_labels = train_data['label']\n",
    "classes = ds['train'].features['label'].names\n",
    "num_classes = len(set(classes))\n",
    "\n",
    "val_images = val_data['image']\n",
    "val_labels = val_data['label']\n",
    "\n",
    "test_images = test_data['image']\n",
    "test_labels = test_data['label']\n",
    "\n",
    "train_images = convert_bytes_to_images(train_images)\n",
    "val_images = convert_bytes_to_images(val_images)\n",
    "test_images = convert_bytes_to_images(test_images)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 for CLIP\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))  # CLIP's mean and std\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom datasets\n",
    "train_dataset = CustomDataset(images=train_images, labels=train_labels, classes=classes, transform=transform)\n",
    "val_dataset = CustomDataset(images=val_images, labels=val_labels, classes=classes, transform=transform)\n",
    "test_dataset = CustomDataset(images=test_images, labels=test_labels, classes=classes, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = transforms.Compose([\n",
    "    transforms.AutoAugment()\n",
    "])\n",
    "\n",
    "rand = transforms.Compose([\n",
    "    transforms.RandAugment()\n",
    "])\n",
    "\n",
    "augM = transforms.Compose([\n",
    "    transforms.AugMix()\n",
    "])\n",
    "\n",
    "trAu = transforms.Compose([\n",
    "    transforms.TrivialAugmentWide()\n",
    "])\n",
    "\n",
    "\n",
    "class GaussianNoise(object):\n",
    "    def __init__(self, mean=0.0, sigma=1.0, noise_ratio=1.0):\n",
    "        self.mean = mean\n",
    "        self.sigma = sigma\n",
    "        self.noise_ratio = noise_ratio\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Convert PIL Image to Tensor\n",
    "        img = transforms.ToTensor()(img)\n",
    "\n",
    "        # Generate Gaussian noise\n",
    "        noise = torch.randn(img.size()) * self.sigma + self.mean\n",
    "\n",
    "        # Apply noise to the image\n",
    "        noisy_img = img + noise * self.noise_ratio\n",
    "\n",
    "        # Clip to maintain valid pixel range\n",
    "        noisy_img = torch.clamp(noisy_img, 0.0, 1.0)\n",
    "\n",
    "        # Convert back to PIL Image\n",
    "        return transforms.ToPILImage()(noisy_img)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"GaussianNoise(mean={self.mean}, sigma={self.sigma}, noise_ratio={self.noise_ratio})\"\n",
    "\n",
    "noise_01 = transforms.Compose([                 # Resizing images\n",
    "    GaussianNoise(mean=0.0, sigma=0.1, noise_ratio=0.1)\n",
    "])\n",
    "\n",
    "\n",
    "noise_03 = transforms.Compose([                 # Resizing images\n",
    "    GaussianNoise(mean=0.0, sigma=0.1, noise_ratio=0.3)\n",
    "])\n",
    "\n",
    "\n",
    "noise_05 = transforms.Compose([                   # Resizing images\n",
    "    GaussianNoise(mean=0.0, sigma=0.1, noise_ratio=0.5)\n",
    "])\n",
    "\n",
    "auto1 = transforms.Compose([\n",
    "    transforms.AutoAugment(),\n",
    "    GaussianNoise(mean=0.0, sigma=0.1, noise_ratio=0.5)\n",
    "])\n",
    "\n",
    "auto2 = transforms.Compose([\n",
    "    transforms.AutoAugment(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    GaussianNoise(mean=0.0, sigma=0.1, noise_ratio=0.5)\n",
    "])\n",
    "\n",
    "auto3 = transforms.Compose([\n",
    "    transforms.AutoAugment(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.ElasticTransform(alpha=40.0, sigma=6.0, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    GaussianNoise(mean=0.0, sigma=0.1, noise_ratio=0.5)\n",
    "])\n",
    "\n",
    "\n",
    "def auto_transform(image):\n",
    "    image = auto(image)  # Apply auto3 transform (assumes this returns a PIL image)\n",
    "    return image\n",
    "\n",
    "def rand_transform(image):\n",
    "    image = rand(image)  # Apply auto3 transform (assumes this returns a PIL image)\n",
    "    return image\n",
    "\n",
    "def augM_transform(image):\n",
    "    image = augM(image)  # Apply auto3 transform (assumes this returns a PIL image)\n",
    "    return image\n",
    "\n",
    "def trAu_transform(image):\n",
    "    image = trAu(image)  # Apply auto3 transform (assumes this returns a PIL image)\n",
    "    return image\n",
    "\n",
    "def noise_01_transform(image):\n",
    "    image = noise_01(image)  # Apply auto3 transform (assumes this returns a PIL image)\n",
    "    return image\n",
    "\n",
    "def noise_03_transform(image):\n",
    "    image = noise_03(image)  # Apply auto3 transform (assumes this returns a PIL image)\n",
    "    return image\n",
    "\n",
    "def noise_05_transform(image):\n",
    "    image = noise_05(image)  # Apply auto3 transform (assumes this returns a PIL image)\n",
    "    return image\n",
    "\n",
    "def auto1_transform(image):\n",
    "    image = auto1(image)  # Apply auto3 transform (assumes this returns a PIL image)\n",
    "    return image\n",
    "\n",
    "def auto2_transform(image):\n",
    "    image = auto2(image)  # Apply auto3 transform (assumes this returns a PIL image)\n",
    "    return image\n",
    "\n",
    "def auto3_transform(image):\n",
    "    image = auto3(image)  # Apply auto3 transform (assumes this returns a PIL image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_div = {'Auto': auto_transform,\n",
    "               'Rand': rand_transform,\n",
    "               'AugM': augM_transform,\n",
    "               'TrAu': trAu_transform,\n",
    "               'Noize10': noise_01_transform,\n",
    "               'Noize30': noise_03_transform,\n",
    "               'Noize50': noise_05_transform,\n",
    "               'Auto1': auto1_transform,\n",
    "               'Auto2': auto2_transform,\n",
    "               'Auto3': auto3_transform\n",
    "               }\n",
    "\n",
    "# Dictionaries to store models, watchers, and summaries\n",
    "model_div = {}\n",
    "ft_model_div = {}\n",
    "trace_div = {}\n",
    "\n",
    "for k, v in dataset_div.items():\n",
    "\n",
    "    # Create custom datasets\n",
    "    train_dataset = CustomDataset(images=train_images, labels=train_labels, classes=classes, transform=transform)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Define models and watchers\n",
    "    model_div[k] = ResNetFinetuner(num_classes=num_classes)\n",
    "\n",
    "    # Save model state\n",
    "    torch.save(model_div[k].state_dict(), f'loss_model/model_div{k}.pth')\n",
    "\n",
    "    # Train the model\n",
    "    ft_model_div[k], trace_div[k] = eigen_train(model_div[k], train_dataloader, num_epochs=epochs)\n",
    "\n",
    "    # Save fine-tuned model state\n",
    "    torch.save(ft_model_div[k].state_dict(), f'loss_model/ft_model_div{k}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_div = pd.DataFrame([trace_div])\n",
    "\n",
    "trace_div.to_csv('loss_model/trace_div.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_bs_df = pd.read_csv('loss_model/trace_bs.csv')\n",
    "\n",
    "trace_dp_df = pd.read_csv('loss_model/trace_dp.csv')\n",
    "\n",
    "trace_wd_df = pd.read_csv('loss_model/trace_wd.csv')\n",
    "\n",
    "trace_div_df = pd.read_csv('loss_model/trace_div.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_bs = np.array(trace_bs_df)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "dp01 = ast.literal_eval(trace_dp_df.iloc[:, 0][0])\n",
    "dp03 = ast.literal_eval(trace_dp_df.iloc[:, 1][0])\n",
    "dp05 = ast.literal_eval(trace_dp_df.iloc[:, 2][0])\n",
    "dp07 = ast.literal_eval(trace_dp_df.iloc[:, 3][0])\n",
    "\n",
    "wd1e_5 = ast.literal_eval(trace_wd_df.iloc[:, 0][0])\n",
    "wd5e_5 = ast.literal_eval(trace_wd_df.iloc[:, 1][0])\n",
    "wd1e_4 = ast.literal_eval(trace_wd_df.iloc[:, 2][0])\n",
    "wd5e_4 = ast.literal_eval(trace_wd_df.iloc[:, 3][0])\n",
    "wd1e_3 = ast.literal_eval(trace_wd_df.iloc[:, 4][0])\n",
    "wd5e_3 = ast.literal_eval(trace_wd_df.iloc[:, 5][0])\n",
    "\n",
    "div01 = ast.literal_eval(trace_div_df.iloc[:, 0][0])\n",
    "div02 = ast.literal_eval(trace_div_df.iloc[:, 1][0])\n",
    "div03 = ast.literal_eval(trace_div_df.iloc[:, 2][0])\n",
    "div04 = ast.literal_eval(trace_div_df.iloc[:, 3][0])\n",
    "div05 = ast.literal_eval(trace_div_df.iloc[:, 4][0])\n",
    "div06 = ast.literal_eval(trace_div_df.iloc[:, 5][0])\n",
    "div07 = ast.literal_eval(trace_div_df.iloc[:, 6][0])\n",
    "div08 = ast.literal_eval(trace_div_df.iloc[:, 7][0])\n",
    "div09 = ast.literal_eval(trace_div_df.iloc[:, 8][0])\n",
    "div10 = ast.literal_eval(trace_div_df.iloc[:, 9][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(scalars, weight):  # Weight between 0 and 1\n",
    "    last = scalars[0]  # First value in the plot (first timestep)\n",
    "    smoothed = list()\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n",
    "        smoothed.append(smoothed_val)                        # Save it\n",
    "        last = smoothed_val                                  # Anchor the last smoothed value\n",
    "        \n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "colours = [\"#000000\", \"#E69F00\", \"#0072B2\", \"#009E73\", \"#CC79A7\"]\n",
    "plt.plot(np.arange(480), smooth(trace_bs, 0.98), label='baseline', c=colours[0])\n",
    "plt.plot(np.arange(480), smooth(dp01, 0.98), label='dropout_01', c=colours[1])\n",
    "plt.plot(np.arange(480), smooth(dp03, 0.98), label='dropout_03', c=colours[2])\n",
    "plt.plot(np.arange(480), smooth(dp05, 0.98), label='dropout_05', c=colours[3])\n",
    "plt.plot(np.arange(480), smooth(dp07, 0.98), label='dropout_07', c=colours[4])\n",
    "plt.ylabel(\"Trace\", fontsize=12)\n",
    "plt.xlabel(\"Iteration\", fontsize=12)\n",
    "plt.axvline(x=245, color='black', linestyle=':')\n",
    "plt.axvline(x=375, color='black', linestyle=':')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"loss_dp.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "colours = [\"#000000\", \"#E69F00\", \"#0072B2\", \"#009E73\", \"#CC79A7\", \"#D55E00\"]\n",
    "plt.plot(np.arange(480), smooth(trace_bs, 0.98), label='baseline', c=colours[0])\n",
    "plt.plot(np.arange(480), smooth(dp01, 0.98), label='dropout_01', c=colours[1])\n",
    "plt.plot(np.arange(480), smooth(wd5e_3, 0.98), label='weight decay_5e-3', c=colours[2])\n",
    "plt.plot(np.arange(480), smooth(div04, 0.98), label='TrAu', c=colours[3])\n",
    "plt.plot(np.arange(480), smooth(div06, 0.98), label='Noise_03', c=colours[4])\n",
    "plt.ylabel(\"Trace\", fontsize=12)\n",
    "plt.xlabel(\"Iteration\", fontsize=12)\n",
    "plt.axvline(x=245, color='black', linestyle=':')\n",
    "plt.axvline(x=375, color='black', linestyle=':')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"loss_vis.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "colours = [\"#000000\", \"#E69F00\", \"#0072B2\", \"#009E73\", \"#CC79A7\", \"#56B4E9\", \"#F0E442\", \"#D55E00\", \"#4B0092\", \"#A52A2A\", \"#008000\"]\n",
    "plt.plot(np.arange(480), smooth(trace_bs, 0.98), label='baseline', c=colours[0])\n",
    "plt.plot(np.arange(480), smooth(wd1e_5, 0.98), label='weight decay_1e_5', c=colours[1])\n",
    "plt.plot(np.arange(480), smooth(wd5e_5, 0.98), label='weight decay_5e_5', c=colours[2])\n",
    "plt.plot(np.arange(480), smooth(wd1e_4, 0.98), label='weight decay_1e_4', c=colours[3])\n",
    "plt.plot(np.arange(480), smooth(wd5e_4, 0.98), label='weight decay_5e_4', c=colours[4])\n",
    "plt.plot(np.arange(480), smooth(wd1e_3, 0.98), label='weight decay_1e_3', c=colours[5])\n",
    "plt.plot(np.arange(480), smooth(wd5e_3, 0.98), label='weight decay_5e_3', c=colours[6])\n",
    "plt.ylabel(\"Trace\", fontsize=12)\n",
    "plt.xlabel(\"Iteration\", fontsize=12)\n",
    "plt.axvline(x=245, color='black', linestyle=':')\n",
    "plt.axvline(x=375, color='black', linestyle=':')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"loss_wd.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "colours = [\"#000000\", \"#E69F00\", \"#0072B2\", \"#009E73\", \"#CC79A7\", \"#56B4E9\", \"#F0E442\", \"#D55E00\", \"#4B0092\", \"#A52A2A\", \"#008000\"]\n",
    "plt.plot(np.arange(480), smooth(trace_bs, 0.98), label='baseline', c=colours[0])\n",
    "plt.plot(np.arange(480), smooth(div01, 0.98), label='Auto', c=colours[1])\n",
    "plt.plot(np.arange(480), smooth(div02, 0.98), label='Rand', c=colours[2])\n",
    "plt.plot(np.arange(480), smooth(div03, 0.98), label='AugM', c=colours[3])\n",
    "plt.plot(np.arange(480), smooth(div04, 0.98), label='TrAu', c=colours[4])\n",
    "plt.plot(np.arange(480), smooth(div05, 0.98), label='Noise10', c=colours[5])\n",
    "plt.plot(np.arange(480), smooth(div06, 0.98), label='Noise30', c=colours[6])\n",
    "plt.plot(np.arange(480), smooth(div07, 0.98), label='Nosie50', c=colours[7])\n",
    "plt.plot(np.arange(480), smooth(div08, 0.98), label='Auto-v1', c=colours[8])\n",
    "plt.plot(np.arange(480), smooth(div09, 0.98), label='Auto-v2', c=colours[9])\n",
    "plt.plot(np.arange(480), smooth(div09, 0.98), label='Auto-v3', c=colours[9])\n",
    "plt.ylabel(\"Trace\", fontsize=12)\n",
    "plt.xlabel(\"Iteration\", fontsize=12)\n",
    "plt.axvline(x=245, color='black', linestyle=':')\n",
    "plt.axvline(x=375, color='black', linestyle=':')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"loss_div.png\", dpi=300, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diversity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
