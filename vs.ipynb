{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd0f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74972b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.linalg\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def weight_K(K, p=None):\n",
    "    if p is None:\n",
    "        return K / K.shape[0]\n",
    "    else:\n",
    "        return K * np.outer(np.sqrt(p), np.sqrt(p))\n",
    "\n",
    "\n",
    "def normalize_K(K):\n",
    "    d = np.sqrt(np.diagonal(K))\n",
    "    return K / np.outer(d, d)\n",
    "\n",
    "\n",
    "def entropy_q(p, q=1):\n",
    "    p_ = p[p > 0]\n",
    "    if q == 1:\n",
    "        return -(p_ * np.log(p_)).sum()\n",
    "    if q == \"inf\":\n",
    "        return -np.log(np.max(p))\n",
    "    return np.log((p_ ** q).sum()) / (1 - q)\n",
    "\n",
    "\n",
    "def score_K(K, q=1, p=None, normalize=False):\n",
    "    if normalize:\n",
    "        K = normalize_K(K)\n",
    "    K_ = weight_K(K, p)\n",
    "    if type(K_) == scipy.sparse.csr.csr_matrix:\n",
    "        w, _ = scipy.sparse.linalg.eigsh(K_)\n",
    "    else:\n",
    "        w = scipy.linalg.eigvalsh(K_)\n",
    "    vendi = np.exp(entropy_q(w, q=q))\n",
    "    return vendi, w\n",
    "\n",
    "\n",
    "def score_X(X, q=1, p=None, normalize=True):\n",
    "    if normalize:\n",
    "        X = preprocessing.normalize(X, axis=1)\n",
    "    K = X @ X.T\n",
    "    return score_K(K, q=1, p=p)\n",
    "\n",
    "\n",
    "def score_dual(X, q=1, normalize=True):\n",
    "    if normalize:\n",
    "        X = preprocessing.normalize(X, axis=1)\n",
    "    n = X.shape[0]\n",
    "    S = X.T @ X\n",
    "    w = scipy.linalg.eigvalsh(S / n)\n",
    "    vendi = np.exp(entropy_q(w, q=q))\n",
    "    m = w > 0\n",
    "    return vendi, w\n",
    "\n",
    "\n",
    "def score(samples, k, q=1, p=None, normalize=False):\n",
    "    n = len(samples)\n",
    "    K = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            K[i, j] = K[j, i] = k(samples[i], samples[j])\n",
    "    return score_K(K, p=p, q=q, normalize=normalize)\n",
    "\n",
    "\n",
    "def intdiv_K(K, q=1, p=None):\n",
    "    K_ = K ** q\n",
    "    if p is None:\n",
    "        p = np.ones(K.shape[0]) / K.shape[0]\n",
    "    return 1 - np.sum(K_ * np.outer(p, p))\n",
    "\n",
    "\n",
    "def intdiv_X(X, q=1, p=None, normalize=True):\n",
    "    if normalize:\n",
    "        X = preprocessing.normalize(X, axis=1)\n",
    "    K = X @ X.T\n",
    "    return intdiv(K, q=q, p=p)\n",
    "\n",
    "\n",
    "def intdiv(samples, k, q=1, p=None):\n",
    "    n = len(samples)\n",
    "    K = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            K[i, j] = K[j, i] = k(samples[i], samples[j])\n",
    "    return intdiv_K(K, q=q, p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e56f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import datasets\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate import bleu_score\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from vendi_score import data_utils, vendi\n",
    "from vendi_score.data_utils import Example, Group\n",
    "\n",
    "\n",
    "def get_tokenizer(model=\"roberta-base\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", use_fast=True)\n",
    "\n",
    "    def tokenize(s):\n",
    "        return tokenizer.convert_ids_to_tokens(tokenizer(s).input_ids)\n",
    "\n",
    "    return tokenize\n",
    "\n",
    "\n",
    "def sklearn_tokenizer():\n",
    "    return CountVectorizer().build_tokenizer()\n",
    "\n",
    "\n",
    "def get_mnli():\n",
    "    data = itertools.chain(\n",
    "        datasets.load_dataset(\"multi_nli\", split=\"validation_matched\"),\n",
    "        datasets.load_dataset(\"multi_nli\", split=\"validation_mismatched\"),\n",
    "    )\n",
    "    seen = set()\n",
    "    examples = []\n",
    "    for d in data:\n",
    "        s = d[\"premise\"]\n",
    "        if s in seen:\n",
    "            continue\n",
    "        seen.add(s)\n",
    "        examples.append(Example(x=s, labels={\"y\": d[\"genre\"]}))\n",
    "    return examples\n",
    "\n",
    "\n",
    "def get_ngrams(\n",
    "    sents,\n",
    "    n=1,\n",
    "    tokenizer=None,\n",
    "    return_vectorizer=False,\n",
    "    lowercase=False,\n",
    "    **kwargs,\n",
    "):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = word_tokenize\n",
    "    ngram_range = n if type(n) == tuple else (n, n)\n",
    "    vectorizer = CountVectorizer(\n",
    "        tokenizer=tokenizer,\n",
    "        ngram_range=ngram_range,\n",
    "        lowercase=lowercase,\n",
    "        **kwargs,\n",
    "    )\n",
    "    X = vectorizer.fit_transform(sents)\n",
    "    if return_vectorizer:\n",
    "        return X, vectorizer\n",
    "    return X\n",
    "\n",
    "\n",
    "def add_ngrams_to_examples(\n",
    "    examples, n=1, tokenizer=None, return_vectorizer=False, **kwargs\n",
    "):\n",
    "    X = get_ngrams([e.x for e in examples], n=n, tokenizer=tokenizer, **kwargs)\n",
    "    for e, x in zip(examples, X):\n",
    "        e.features[f\"{n}-grams\"] = x\n",
    "    return examples\n",
    "\n",
    "\n",
    "def get_embeddings(\n",
    "    sents,\n",
    "    model=None,\n",
    "    tokenizer=None,\n",
    "    batch_size=32,\n",
    "    device=\"cpu\",\n",
    "    model_path=\"bert-base-uncased\"\n",
    "):\n",
    "    # Handle device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "\n",
    "    # Load model/tokenizer if not provided\n",
    "    if model is None:\n",
    "        model = AutoModel.from_pretrained(model_path).eval().to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    # Helper: batching\n",
    "    def to_batches(data, batch_size):\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            yield data[i : i + batch_size]\n",
    "\n",
    "    for batch in to_batches(sents, batch_size):\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs)\n",
    "\n",
    "            # Use pooler_output if available; fallback to CLS token\n",
    "            if hasattr(output, \"pooler_output\") and output.pooler_output is not None:\n",
    "                pooled = output.pooler_output  # [batch_size, hidden_dim]\n",
    "            else:\n",
    "                pooled = output.last_hidden_state[:, 0]  # [batch_size, hidden_dim]\n",
    "\n",
    "        # No reshape here â€” each batch is already 2D\n",
    "        embeddings.append(pooled.cpu().numpy())\n",
    "\n",
    "    # Concatenate all batches: [total_samples, hidden_dim]\n",
    "    return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "\n",
    "def add_embeddings_to_examples(\n",
    "    examples,\n",
    "    model=None,\n",
    "    tokenizer=None,\n",
    "    batch_size=32,\n",
    "    device=\"cpu\",\n",
    "    model_name=\"princeton-nlp/unsup-simcse-roberta-base\",\n",
    "    feature_name=\"unsup_simcse\",\n",
    "):\n",
    "    X = get_embeddings(\n",
    "        [e.x for e in examples],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        model_name=model_name,\n",
    "    )\n",
    "    for e, x in zip(examples, X):\n",
    "        e.features[feature_name] = x\n",
    "    return examples\n",
    "\n",
    "\n",
    "def single_ngram_diversity(sents, n, tokenizer=None, **kwargs):\n",
    "    X = get_ngrams(sents, n=n, tokenizer=tokenizer, **kwargs)\n",
    "    distinct = X.shape[-1]\n",
    "    total = X.sum()\n",
    "    # unique = (counts == 1).sum()\n",
    "    # total = counts.shape[-1]\n",
    "    return distinct / total\n",
    "\n",
    "\n",
    "def ngram_diversity(sents, ns=[1, 2, 3, 4], tokenizer=None, **kwargs):\n",
    "    return np.mean(\n",
    "        [\n",
    "            single_ngram_diversity(sents, n, tokenizer=tokenizer, **kwargs)\n",
    "            for n in ns\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def bleu(hyps, refs, tokenizer=None):\n",
    "    if type(hyps[0]) == str:\n",
    "        if tokenizer is None:\n",
    "            tokenizer = word_tokenize\n",
    "        hyp_tokens = [tokenizer(s) for s in hyps]\n",
    "        ref_tokens = [tokenizer(s) for s in refs]\n",
    "    else:\n",
    "        hyp_tokens = hyps\n",
    "        ref_tokens = refs\n",
    "    smoothing = bleu_score.SmoothingFunction().method1\n",
    "    return np.mean(\n",
    "        [\n",
    "            bleu_score.sentence_bleu(refs, hyp, smoothing_function=smoothing)\n",
    "            for hyp in hyps\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def self_bleu(sents, tokenizer):\n",
    "    examples = [tokenizer(s) for s in sents]\n",
    "    smoothing = bleu_score.SmoothingFunction().method1\n",
    "    scores = []\n",
    "    for i in range(len(examples)):\n",
    "        hyp = examples[i]\n",
    "        ref = examples[:i] + examples[i + 1 :]\n",
    "        scores.append(\n",
    "            bleu_score.sentence_bleu(ref, hyp, smoothing_function=smoothing)\n",
    "        )\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def pairwise_bleu(sents, tokenizer):\n",
    "    examples = [tokenizer(s) for s in sents]\n",
    "    smoothing = bleu_score.SmoothingFunction().method1\n",
    "    scores = []\n",
    "    for i in range(len(examples)):\n",
    "        lst = []\n",
    "        for j in range(len(examples)):\n",
    "            if j == i:\n",
    "                continue\n",
    "            hyp = examples[i]\n",
    "            ref = [examples[j]]\n",
    "            lst.append(\n",
    "                bleu_score.sentence_bleu(ref, hyp, smoothing_function=smoothing)\n",
    "            )\n",
    "        scores.append(np.mean(lst))\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def ngram_vendi_score(sents, ns=[1, 2, 3, 4], tokenizer=None, **kwargs):\n",
    "    Ks = []\n",
    "    for n in ns:\n",
    "        X = normalize(get_ngrams(sents, n=n, tokenizer=tokenizer))\n",
    "        Ks.append((X @ X.T).A)\n",
    "    K = np.stack(Ks, axis=0).mean(axis=0)\n",
    "    return vendi.score_K(K)\n",
    "\n",
    "\n",
    "def embedding_vendi_score(\n",
    "    sents,\n",
    "    model=None,\n",
    "    tokenizer=None,\n",
    "    batch_size=32,\n",
    "    device=\"cpu\",\n",
    "    model_path=\"princeton-nlp/unsup-simcse-roberta-base\",\n",
    "):\n",
    "    X = get_embeddings(\n",
    "        sents,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        model_path=model_path,\n",
    "    )\n",
    "    n, d = X.shape\n",
    "    if n < d:\n",
    "        s, w = score_X(X)\n",
    "        return s, w\n",
    "    s, w = score_dual(X)\n",
    "    return s, w, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('augment_training/df_train_tc.csv')\n",
    "df_val = pd.read_csv('augment_training/df_val_tc.csv')\n",
    "df_test = pd.read_csv('augment_training/df_test_tc.csv')\n",
    "\n",
    "df_gen = pd.read_csv('augment_training/df_gen_tc.csv')\n",
    "df_gen['text'] = df_gen['text'].astype(str)\n",
    "\n",
    "\n",
    "df_gen_01 = pd.read_csv('augment_training/df_comb_tc_0.1.csv')\n",
    "df_gen_03 = pd.read_csv('augment_training/df_comb_tc_0.3.csv')\n",
    "df_gen_05 = pd.read_csv('augment_training/df_comb_tc_0.5.csv')\n",
    "df_gen_07 = pd.read_csv('augment_training/df_comb_tc_0.7.csv')\n",
    "df_gen_09 = pd.read_csv('augment_training/df_comb_tc_0.9.csv')\n",
    "\n",
    "df_gen_01_vf = pd.read_csv('augment_training/df_comb_tc_0.1_vf.csv')\n",
    "df_gen_03_vf = pd.read_csv('augment_training/df_comb_tc_0.3_vf.csv')\n",
    "df_gen_05_vf = pd.read_csv('augment_training/df_comb_tc_0.5_vf.csv')\n",
    "df_gen_07_vf = pd.read_csv('augment_training/df_comb_tc_0.7_vf.csv')\n",
    "df_gen_09_vf = pd.read_csv('augment_training/df_comb_tc_0.9_vf.csv')\n",
    "df_gen_vf = pd.read_csv('augment_training/df_gen_tc_vf.csv')\n",
    "\n",
    "df_gen_01_add = pd.read_csv('augment_training/df_comb_tc_0.1_add.csv')\n",
    "df_gen_03_add = pd.read_csv('augment_training/df_comb_tc_0.3_add.csv')\n",
    "df_gen_05_add = pd.read_csv('augment_training/df_comb_tc_0.5_add.csv')\n",
    "df_gen_07_add = pd.read_csv('augment_training/df_comb_tc_0.7_add.csv')\n",
    "df_gen_09_add = pd.read_csv('augment_training/df_comb_tc_0.9_add.csv')\n",
    "df_gen_add = pd.read_csv('augment_training/df_gen_tc_add.csv')\n",
    "\n",
    "df_div_ri_03 = pd.read_csv('EDA_aug/augmented_ri_augp0.3_prop0.3.csv')\n",
    "df_div_sr_03 = pd.read_csv('EDA_aug/augmented_sr_augp0.3_prop0.3.csv')\n",
    "df_div_rs_03 = pd.read_csv('EDA_aug/augmented_rs_augp0.3_prop0.3.csv')\n",
    "df_div_rd_03 = pd.read_csv('EDA_aug/augmented_rd_augp0.3_prop0.3.csv')\n",
    "\n",
    "df_div_ri_05 = pd.read_csv('EDA_aug/augmented_ri_augp0.5_prop0.5.csv')\n",
    "df_div_sr_05 = pd.read_csv('EDA_aug/augmented_sr_augp0.5_prop0.5.csv')\n",
    "df_div_rs_05 = pd.read_csv('EDA_aug/augmented_rs_augp0.5_prop0.5.csv')\n",
    "df_div_rd_05 = pd.read_csv('EDA_aug/augmented_rd_augp0.5_prop0.5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d7828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vendi_score import text_utils\n",
    "\n",
    "vs = {}\n",
    "ngm = {}\n",
    "eigen = {}\n",
    "X = {}\n",
    "\n",
    "results = []\n",
    "\n",
    "data = [\n",
    "    (\"bs\", df_train),\n",
    "    (\"gen_01\", df_gen_01),\n",
    "    (\"gen_03\", df_gen_03),\n",
    "    (\"gen_05\", df_gen_05),\n",
    "    (\"gen_07\", df_gen_07),\n",
    "    (\"gen_09\", df_gen_09),\n",
    "    (\"gen\", df_gen), \n",
    "    (\"gen_01_vf\", df_gen_01_vf),\n",
    "    (\"gen_03_vf\", df_gen_03_vf),\n",
    "    (\"gen_05_vf\", df_gen_05_vf),\n",
    "    (\"gen_07_vf\", df_gen_07_vf),\n",
    "    (\"gen_09_vf\", df_gen_09_vf),\n",
    "    (\"gen_vf\", df_gen_vf), \n",
    "    (\"gen_01_add\", df_gen_01_add),\n",
    "    (\"gen_03_add\", df_gen_03_add),\n",
    "    (\"gen_05_add\", df_gen_05_add),\n",
    "    (\"gen_07_add\", df_gen_07_add),\n",
    "    (\"gen_09_add\", df_gen_09_add),\n",
    "    (\"gen_add\", df_gen_add), \n",
    "    (\"div_ri_03\", df_div_ri_03), \n",
    "    (\"div_sr_03\", df_div_sr_03), \n",
    "    (\"div_rs_03\", df_div_rs_03),\n",
    "    (\"div_rd_03\", df_div_rd_03), \n",
    "    (\"div_ri_05\", df_div_ri_05), \n",
    "    (\"div_sr_05\", df_div_sr_05), \n",
    "    (\"div_rs_05\", df_div_rs_05),\n",
    "    (\"div_rd_05\", df_div_rd_05) \n",
    "    ]\n",
    "\n",
    "for name, df in data:\n",
    "    print(name)\n",
    "\n",
    "    pxl = text_utils.ngram_vendi_score(df['text'].tolist(), ns=[1])\n",
    "    emb, egn, x = embedding_vendi_score(df['text'].tolist(), model_path=\"bert-base-uncased\", device='cuda', batch_size=32)\n",
    "    eigen[name]= egn\n",
    "    ngm[name] = pxl\n",
    "    vs[name] = emb\n",
    "    X[name] = x\n",
    "\n",
    "    print(f\"Pixel Vendi Score: {pxl:.4f}, Embedding Vendi Score: {emb:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"name\": name,\n",
    "        \"pxl\": pxl,\n",
    "        \"emb\": emb\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0ad62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = {\n",
    "    'name': ['bs', 'gen', 'gen_01', 'gen_03', 'gen_05', 'gen_07','gen_09'], \n",
    "    'acc': [0.875, 0.835, 0.870, 0.852, 0.884, 0.884, 0.890]\n",
    "}\n",
    "\n",
    "\n",
    "div_03 = {\n",
    "    'name': ['div_sr_03', 'div_rd_03', 'div_ri_03', 'div_rs_03'],\n",
    "    'acc': [0.887,  0.870, 0.907, 0.899]\n",
    "}\n",
    "\n",
    "\n",
    "div_05 = {\n",
    "    'name': ['div_sr_05', 'div_rd_05', 'div_ri_05', 'div_rs_05'],\n",
    "    'acc': [0.907, 0.870, 0.893,  0.890]\n",
    "    }\n",
    "\n",
    "\n",
    "gen_vf = {\n",
    "    'name': ['gen_01_vf', 'gen_03_vf', 'gen_05_vf', 'gen_07_vf', 'gen_09_vf', 'gen_vf'],\n",
    "    'acc': [0.867, 0.887, 0.890,  0.890, 0.881, 0.864]\n",
    "    }\n",
    "\n",
    "\n",
    "gen_add = {\n",
    "    'name': ['gen_01_add', 'gen_03_add', 'gen_05_add', 'gen_07_add', 'gen_09_add', 'gen_add'],\n",
    "    'acc': [0.864, 0.896, 0.916,  0.884, 0.907,  0.904]\n",
    "    }\n",
    "\n",
    "df_bs = pd.DataFrame(bs)\n",
    "df_div03 = pd.DataFrame(div_03)\n",
    "df_div05 = pd.DataFrame(div_05)\n",
    "df_gen_vf = pd.DataFrame(gen_vf)\n",
    "df_gen_add = pd.DataFrame(gen_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55225d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = pd.concat([df_bs, df_div03, df_div05, df_gen_vf, df_gen_add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f36ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_results, df_acc, on='name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae2393",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = []\n",
    "for x in X.keys(): \n",
    "    if x.endswith('_add'): \n",
    "        base  = X['bs']                 # (2759, 768)\n",
    "        other = X[x]                     # (2758, 768)\n",
    "        # --- keep only the first 2758 rows of the baseline ---\n",
    "        n = other.shape[0] - base.shape[0] # 2758\n",
    "        other = other[base.shape[0]:]\n",
    "        base  = base[:n]\n",
    "\n",
    "        r = np.sum(base * other) / (\n",
    "                np.linalg.norm(base) * np.linalg.norm(other)\n",
    "            )\n",
    "        rho.append(r)\n",
    "    else: \n",
    "        base  = X['bs']                 # (2759, 768)\n",
    "        other = X[x]                    # (2758, 768)\n",
    "        # --- keep only the first 2758 rows of the baseline ---\n",
    "        n = min(base.shape[0], other.shape[0])   # 2758\n",
    "        base  = base[:n]\n",
    "        other = other[:n]\n",
    "\n",
    "        r = np.sum(base * other) / (\n",
    "                np.linalg.norm(base) * np.linalg.norm(other)\n",
    "            )\n",
    "        rho.append(r)\n",
    "\n",
    "df['rho'] = rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = ['bs'] + 6*['gen'] + 6*['gen_vf'] + 6*['gen_add'] + 8*['div']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c41baef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wvs'] = df['emb']*df['rho']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbcc07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['category','emb', 'acc', 'rho', 'wvs']].groupby('category').mean().round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8014918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pp'] = [0.5] + [0.9, 0.7, 0.5, 0.3, 0.1, 1] + [0.9, 0.7, 0.5, 0.3, 0.1, 1] + [0.1, 0.3, 0.5, 0.7, 0.9, 1] + 8*[0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce96028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create the plot with no legend\n",
    "plot = sns.scatterplot(\n",
    "    data=df,\n",
    "    x='wvs',\n",
    "    y='acc',\n",
    "    hue='category',\n",
    "    size='pp',\n",
    "    marker='o',\n",
    "    alpha=0.9,\n",
    "    sizes=(30, 100),\n",
    "    linewidth=2,\n",
    "    legend=False  # disable all legends initially\n",
    ")\n",
    "\n",
    "# Get unique categories and their color mappings\n",
    "handles, labels = plot.get_legend_handles_labels()\n",
    "# Seaborn disables legends from scatterplot, so we re-plot the legend for hue only\n",
    "from matplotlib.lines import Line2D\n",
    "unique_categories = df['category'].unique()\n",
    "palette = sns.color_palette(n_colors=len(unique_categories))\n",
    "custom_lines = [Line2D([0], [0], marker='o', color='w', label=cat,\n",
    "                       markerfacecolor=col, markersize=8)\n",
    "                for cat, col in zip(unique_categories, palette)]\n",
    "plt.legend(handles=custom_lines, title='Category')\n",
    "\n",
    "# Reference lines\n",
    "plt.axhline(df[df['category'] == 'bs']['acc'].values, color='black', linestyle='--', linewidth=1)\n",
    "plt.axvline(df[df['category'] == 'bs']['wvs'].values, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "# Final styling\n",
    "plt.xlabel(\"Weighted VS\", fontsize=12)\n",
    "plt.ylabel(\"ACC\", fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diversity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
